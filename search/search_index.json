{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projeto Sistema de Transportadora","text":""},{"location":"#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este projeto foi desenvolvido para criar um sistema de gest\u00e3o inteligente para uma transportadora. Utilizando t\u00e9cnicas avan\u00e7adas de Engenharia de Dados, buscamos otimizar opera\u00e7\u00f5es log\u00edsticas, melhorar a efici\u00eancia das entregas e gerenciar processos internos de maneira mais eficaz. Nossa equipe utilizou ferramentas modernas e tecnologias como Python, PySpark, Delta Lake e Databricks para construir uma robusta pipeline de dados.</p>"},{"location":"#objetivo","title":"Objetivo","text":"<p>Nosso objetivo principal \u00e9 desenvolver um sistema de gest\u00e3o integrado para uma transportadora, usando engenharia de dados para otimizar as opera\u00e7\u00f5es log\u00edsticas. Isso inclui a coleta, armazenamento, transforma\u00e7\u00e3o e an\u00e1lise de dados. Queremos melhorar a efici\u00eancia na atribui\u00e7\u00e3o de motoristas e ve\u00edculos, planejamento de rotas, gest\u00e3o de cargas e oferecer uma melhor experi\u00eancia para os clientes, garantindo entregas pontuais e seguras.</p>"},{"location":"#equipe","title":"Equipe","text":"<p>A equipe respons\u00e1vel pelo desenvolvimento deste projeto inclui:</p> <ul> <li>Henrique Forgiarini - Coleta e integra\u00e7\u00e3o de dados de rastreamento - Perfil GitHub</li> <li>Bruno Supriano - Armazenamento e gerenciamento de dados no Delta Lake - Perfil GitHub</li> <li>Renato Ribas - Configura\u00e7\u00e3o e otimiza\u00e7\u00e3o do ambiente no Databricks - Perfil GitHub</li> <li>Tiago Salles - Desenvolvimento de modelos preditivos para gest\u00e3o de frota - Perfil GitHub</li> <li>Jo\u00e3o Pedro Cardoso - An\u00e1lise de dados utilizando PySpark - Perfil GitHub</li> <li>Diego Hahn - Orquestra\u00e7\u00e3o de workflows no Databricks - Perfil GitHub</li> <li>Jhayne Henemam - Desenvolvimento de dashboards para visualiza\u00e7\u00e3o de dados - Perfil GitHub</li> <li>Keniel Alves - Gerenciamento de versionamento e documenta\u00e7\u00e3o - Perfil GitHub</li> </ul>"},{"location":"#pipeline-de-engenharia-de-dados","title":"Pipeline de Engenharia de Dados","text":"<p>A pipeline desenvolvida neste projeto inclui as seguintes etapas:</p> <ol> <li>Coleta de Dados: Utiliza\u00e7\u00e3o de uma biblioteca de inser\u00e7\u00e3o de dados fict\u00edcios (Faker).</li> <li>Armazenamento de Dados: Utiliza\u00e7\u00e3o do Delta Lake para armazenar dados de forma escal\u00e1vel e confi\u00e1vel.</li> <li>Transforma\u00e7\u00e3o de Dados: Processamento de grandes volumes de dados utilizando PySpark no Databricks.</li> <li>An\u00e1lise de Dados: Utiliza\u00e7\u00e3o de notebooks interativos no Databricks para an\u00e1lise preditiva e otimiza\u00e7\u00e3o de rotas.</li> <li>Automa\u00e7\u00e3o e Orquestra\u00e7\u00e3o: Orquestra\u00e7\u00e3o de workflows no Databricks para automatizar processos de ingest\u00e3o, transforma\u00e7\u00e3o e an\u00e1lise de dados.</li> </ol>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<p>Al\u00e9m das tecnologias mencionadas, este projeto tamb\u00e9m fez uso extensivo das capacidades do Databricks para processamento de big data em tempo real e colabora\u00e7\u00e3o em equipe.</p>"},{"location":"#conclusao","title":"Conclus\u00e3o","text":"<p>Este projeto n\u00e3o apenas demonstra a aplica\u00e7\u00e3o pr\u00e1tica dos conceitos de Engenharia de Dados, mas tamb\u00e9m oferece uma solu\u00e7\u00e3o inovadora para melhorar a efici\u00eancia e a competitividade de uma transportadora. A equipe est\u00e1 entusiasmada em contribuir para um projeto que pode transformar positivamente a log\u00edstica de transporte de cargas.</p> Copyright \u00a9 2024 - SystemTransportDeltaLake - Todos os direitos reservados."},{"location":"about_pipeline/","title":"Oque \u00e9?","text":"<p>Um pipeline de dados \u00e9 um conjunto de processos que extrai dados de v\u00e1rias fontes, transforma esses dados para um formato adequado e os carrega em um sistema de armazenamento, como um banco de dados ou um data warehouse. Os principais componentes de um pipeline de dados s\u00e3o:</p> <ol> <li>Extra\u00e7\u00e3o (Extract): Coleta dados de fontes diversas, como bancos de dados, APIs, arquivos CSV, logs, etc.</li> <li>Transforma\u00e7\u00e3o (Transform): Converte os dados brutos em um formato mais adequado para an\u00e1lise. Isso pode incluir limpeza de dados, agrega\u00e7\u00e3o, normaliza\u00e7\u00e3o, filtragem, enriquecimento, etc.</li> <li>Carregamento (Load): Carrega os dados transformados para o sistema de destino, como um data warehouse, data lake, ou banco de dados anal\u00edtico.</li> </ol>"},{"location":"about_pipeline/#arquitetura-medalhao","title":"Arquitetura Medalh\u00e3o","text":"<p>A arquitetura medalh\u00e3o \u00e9 um padr\u00e3o de design utilizado em data lakes para organizar e gerenciar dados em diferentes camadas de qualidade e maturidade. Ela geralmente \u00e9 composta por tr\u00eas camadas principais:</p> Bronze <p>A camada bronze \u00e9 onde os dados brutos s\u00e3o armazenados. Esses dados s\u00e3o ingeridos de v\u00e1rias fontes sem muitas transforma\u00e7\u00f5es, mantendo sua forma original. A ideia \u00e9 preservar a integridade e a granularidade dos dados originais para que possam ser revisitados, se necess\u00e1rio.</p> <pre><code>{\n    \"data_source\": \"API\",\n    \"data_type\": \"raw\",\n    \"timestamp\": \"2024-06-28T12:00:00Z\"\n}\n</code></pre> Silver <p>Na camada silver, os dados da camada bronze s\u00e3o transformados, limpos e estruturados. Esta camada inclui processos de deduplica\u00e7\u00e3o, formata\u00e7\u00e3o, limpeza de dados, e outras transforma\u00e7\u00f5es necess\u00e1rias para tornar os dados mais \u00fateis e consistentes. A camada silver \u00e9 frequentemente usada para an\u00e1lises explorat\u00f3rias e gera\u00e7\u00e3o de relat\u00f3rios.</p> <pre><code>{\n    \"data_source\": \"API\",\n    \"data_type\": \"cleaned\",\n    \"timestamp\": \"2024-06-28T12:00:00Z\"\n}\n</code></pre> Gold <p>A camada gold cont\u00e9m dados altamente refinados e otimizados para consumo anal\u00edtico. Nesta camada, os dados s\u00e3o agregados, enriquecidos e organizados de forma a atender necessidades espec\u00edficas de an\u00e1lise, como dashboards, relat\u00f3rios detalhados e modelos de machine learning. A camada gold \u00e9 usada para fornecer insights de neg\u00f3cios e suporte a decis\u00f5es estrat\u00e9gicas.</p> <pre><code>{\n    \"data_source\": \"API\",\n    \"data_type\": \"aggregated\",\n    \"timestamp\": \"2024-06-28T12:00:00Z\",\n    \"insights\": {\n        \"key_metric\": 1234.56\n    }\n}\n</code></pre>"},{"location":"about_pipeline/#vantagens-da-arquitetura-medalhao","title":"Vantagens da Arquitetura Medalh\u00e3o","text":"<ul> <li> Organiza\u00e7\u00e3o: Ajuda a estruturar e organizar os dados em um data lake, facilitando a gest\u00e3o e o acesso.</li> <li> Flexibilidade: Permite que dados brutos sejam preservados, enquanto vers\u00f5es mais refinadas dos dados podem ser criadas conforme necess\u00e1rio.</li> <li> Qualidade dos Dados: Melhora a qualidade dos dados ao aplicar processos de limpeza e transforma\u00e7\u00e3o em etapas distintas.</li> <li> Escalabilidade: Facilita a escalabilidade, uma vez que diferentes camadas podem ser geridas e escaladas independentemente.</li> </ul> <p>Esses conceitos s\u00e3o fundamentais para o gerenciamento eficaz de dados em um ambiente de big data, permitindo que as organiza\u00e7\u00f5es extraiam valor dos dados de forma eficiente e estruturada.</p>"},{"location":"pipeline/","title":"Arquitetura","text":""},{"location":"pipeline/#estrutura-da-pipeline","title":"Estrutura da Pipeline","text":"<p>A pipeline de dados foi organizada em tr\u00eas camadas principais: Bronze, Silver e Gold. Cada camada desempenha um papel espec\u00edfico na prepara\u00e7\u00e3o e transforma\u00e7\u00e3o dos dados.</p>"},{"location":"pipeline/#conectando-azure-adls-gen2-no-databricks","title":"Conectando Azure ADLS Gen2 no Databricks","text":""},{"location":"pipeline/#definindo-storage-account-e-sas-key","title":"Definindo storage account e sas key","text":"<pre><code>storageAccountName = \"sua_storage_account\"\nsasToken = dbutils.secrets.get(scope=\"sas-token\", key=\"sas-token\")\n</code></pre>"},{"location":"pipeline/#definindo-uma-funcao-para-montar-um-adls-com-um-ponto-de-montagem-com-adls-sas","title":"Definindo uma fun\u00e7\u00e3o para montar um ADLS com um ponto de montagem com ADLS SAS","text":"<pre><code>def mount_adls(blobContainerName):\n    try:\n      dbutils.fs.mount(\n        source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n        mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n        extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n      )\n      print(\"OK!\")\n    except Exception as e:\n      print(\"Falha\", e)\n</code></pre>"},{"location":"pipeline/#montando-todos-os-containers","title":"Montando todos os containers","text":"<pre><code>mount_adls('landing-zone')\nmount_adls('bronze')\nmount_adls('silver')\nmount_adls('gold')\n</code></pre>"},{"location":"pipeline/#mostrando-os-pontos-de-montagem-no-cluster-databricks","title":"Mostrando os pontos de montagem no cluster Databricks","text":"<pre><code>display(dbutils.fs.mounts())\n</code></pre>"},{"location":"pipeline_bronze/","title":"Camada Bronze","text":"<p>Na camada Bronze, os dados brutos s\u00e3o coletados e armazenados. Isso inclui informa\u00e7\u00f5es diretamente obtidas das tabelas principais. A integridade e a fidelidade dos dados s\u00e3o preservadas nesta etapa inicial.</p>"},{"location":"pipeline_bronze/#mostrando-todos-os-arquivos-da-camada-landing-zone","title":"Mostrando todos os arquivos da camada landing-zone","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone\"))\n</code></pre>"},{"location":"pipeline_bronze/#gerando-um-dataframe-para-cada-arquivo-a-partir-dos-arquivos-csv-gravado-no-container-landing-zone-do-azure-data-lake-storage","title":"Gerando um dataframe para cada arquivo a partir dos arquivos CSV gravado no container landing-zone do Azure Data Lake Storage","text":"<pre><code>df_agendamentos = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Agendamentos.csv\")\ndf_cargas = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Cargas.csv\")\ndf_clientes = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Clientes.csv\")\ndf_motoristas = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Motoristas.csv\") \ndf_rotas = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Rotas.csv\")\ndf_veiculos = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Veiculos.csv\")\n</code></pre>"},{"location":"pipeline_bronze/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_agendamentos = df_agendamentos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Agendamentos.csv\"))\ndf_cargas = df_cargas.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Cargas.csv\"))\ndf_clientes = df_clientes.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Clientes.csv\"))\ndf_motoristas = df_motoristas.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Motoristas.csv\"))\ndf_rotas = df_rotas.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Rotas.csv\"))\ndf_veiculos = df_veiculos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Veiculos.csv\"))\n</code></pre>"},{"location":"pipeline_bronze/#salvando-os-dataframes-em-delta-lake-formato-de-arquivo-no-data-lake-repositorio-cloud","title":"Salvando os dataframes em delta lake (formato de arquivo) no data lake (repositorio cloud)","text":"<pre><code>df_agendamentos.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/agendamentos\")\ndf_cargas.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/cargas\")\ndf_clientes.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/clientes\")\ndf_motoristas.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/motoristas\")\ndf_rotas.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/rotas\")\ndf_veiculos.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/veiculos\")\n</code></pre>"},{"location":"pipeline_bronze/#verificando-os-dados-gravados-em-delta-na-camada-bronze","title":"Verificando os dados gravados em delta na camada bronze","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/bronze/\"))\n</code></pre>"},{"location":"pipeline_bronze/#lendo-um-exemplo-de-um-delta-lake-para-validar-a-existencia-dos-dados-e-das-colunas-do-metadados","title":"Lendo um exemplo de um delta lake para validar a existencia dos dados e das colunas do metadados","text":"<pre><code>spark.read.format('delta').load(f'/mnt/{storageAccountName}/bronze/veiculos').limit(10).display()\n</code></pre>"},{"location":"pipeline_gold/","title":"Camada Gold","text":""},{"location":"pipeline_gold/#mostrando-todos-os-arquivos-da-camada-silver","title":"Mostrando todos os arquivos da camada silver","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/silver\"))\n</code></pre>"},{"location":"pipeline_gold/#gerando-um-dataframe-dos-delta-lake-no-container-bronze-do-azure-data-lake-storage","title":"Gerando um dataframe dos delta lake no container bronze do Azure Data Lake Storage","text":"<pre><code>df_agendamentos = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/agendamentos\")\ndf_cargas = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/cargas\")\ndf_clientes = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/clientes\")\ndf_motoristas = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/motoristas\")\ndf_rotas = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/rotas\")\ndf_veiculos = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/silver/veiculos\")\n</code></pre>"},{"location":"pipeline_gold/#criando-tabelas-dimensao","title":"Criando tabelas dimens\u00e3o","text":""},{"location":"pipeline_gold/#agendamentos","title":"Agendamentos","text":""},{"location":"pipeline_gold/#cria-tabela","title":"Cria tabela","text":"<pre><code>%sql\nCREATE TABLE IF NOT EXISTS dim_agendamentos (\n    sk_agendamentos BIGINT GENERATED BY DEFAULT AS IDENTITY,\n    data_hora_coleta DATE,\n    data_hora_entrega DATE,\n    veiculo_id INT,\n    carga_id INT\n)\nUSING delta\nLOCATION '/mnt/datalake7a68c04c876ba15d/gold/dim_agendamentos';\n</code></pre>"},{"location":"pipeline_gold/#cria-uma-tabela-temporaria","title":"Cria uma tabela tempor\u00e1ria","text":"<pre><code>df_agendamentos.createOrReplaceTempView(\"temp_agendamentos\")\n</code></pre>"},{"location":"pipeline_gold/#da-merge-nos-dados-da-tabela-dimensional-e-temporaria","title":"Da merge nos dados da tabela dimensional e tempor\u00e1ria","text":"<pre><code>%sql\nMERGE INTO dim_agendamentos AS da\nUSING temp_agendamentos AS ta\nON da.veiculo_id = ta.codigo_veiculo AND da.carga_id = ta.codigo_carga\nWHEN MATCHED AND (\n    da.data_hora_coleta &lt;&gt; ta.data_hora_coleta OR \n    da.data_hora_entrega &lt;&gt; ta.data_hora_entrega\n) THEN\n  UPDATE SET \n    da.data_hora_coleta = ta.data_hora_coleta,\n    da.data_hora_entrega = ta.data_hora_entrega,\n    da.veiculo_id = ta.codigo_veiculo,\n    da.carga_id = ta.codigo_carga\nWHEN NOT MATCHED THEN\n  INSERT (\n    data_hora_coleta, data_hora_entrega, veiculo_id, carga_id\n  )\n  VALUES (\n    ta.data_hora_coleta, ta.data_hora_entrega, ta.codigo_veiculo, ta.codigo_carga\n  );\n</code></pre>"},{"location":"pipeline_gold/#cargas","title":"Cargas","text":""},{"location":"pipeline_gold/#cria-tabela_1","title":"Cria Tabela","text":"<pre><code>%sql\n\nCREATE TABLE IF NOT EXISTS dim_cargas (\n   sk_cargas BIGINT GENERATED BY DEFAULT AS IDENTITY,\n   codigo_carga BIGINT,\n   tipo_carga VARCHAR(50),\n   peso_carga DECIMAL(10, 2),\n   comprimento DECIMAL(10, 2),\n   largura DECIMAL(10, 2),\n   altura DECIMAL(10, 2),\n   data_entrega_prevista DATE\n)\nUSING delta\nLOCATION  '/mnt/datalake7a68c04c876ba15d/gold/dim_cargas';\n</code></pre>"},{"location":"pipeline_gold/#cria-uma-tabela-temporaria_1","title":"Cria uma tabela tempor\u00e1ria","text":"<pre><code>df_cargas.createOrReplaceTempView(\"temp_cargas\")\n</code></pre>"},{"location":"pipeline_gold/#da-merge-nos-dados-da-tabela-dimensional-e-temporaria_1","title":"Da merge nos dados da tabela dimensional e tempor\u00e1ria","text":"<pre><code>%sql\nMERGE INTO dim_cargas AS dim\nUSING temp_cargas AS temp\nON dim.codigo_carga = temp.codigo_carga \nWHEN MATCHED AND (dim.tipo_carga &lt;&gt; temp.tipo_carga OR dim.peso_carga &lt;&gt; temp.peso_carga OR dim.comprimento &lt;&gt; temp.comprimento OR dim.largura &lt;&gt; temp.largura OR dim.altura &lt;&gt; temp.altura OR dim.data_entrega_prevista &lt;&gt; temp.data_entrega_prevista) THEN\n  UPDATE SET \n    dim.tipo_carga = temp.tipo_carga,\n    dim.peso_carga = temp.peso_carga,\n    dim.comprimento = temp.comprimento,\n    dim.largura = temp.largura,\n    dim.altura = temp.altura,\n    dim.data_entrega_prevista = temp.data_entrega_prevista\nWHEN NOT MATCHED THEN\n  INSERT (codigo_carga, tipo_carga, peso_carga, comprimento, largura, altura, data_entrega_prevista)\n  VALUES (temp.codigo_carga, temp.tipo_carga, temp.peso_carga, temp.comprimento, temp.largura, temp.altura, temp.data_entrega_prevista);\n</code></pre>"},{"location":"pipeline_gold/#veiculos","title":"Ve\u00edculos","text":""},{"location":"pipeline_gold/#cria-tabela_2","title":"Cria Tabela","text":"<pre><code>%sql\n\nCREATE TABLE IF NOT EXISTS dim_veiculos (\n   sk_veiculos BIGINT GENERATED BY DEFAULT AS IDENTITY,\n   codigo_veiculo BIGINT,\n   tipo_veiculo VARCHAR(50),\n   data_aquisicao DATE,\n   estado_veiculo VARCHAR(50),\n   placa_veiculo VARCHAR(10)\n)\nUSING delta\nLOCATION  '/mnt/datalake7a68c04c876ba15d/gold/dim_veiculos';\n</code></pre>"},{"location":"pipeline_gold/#cria-tabela-temporaria","title":"Cria Tabela Tempor\u00e1ria","text":"<pre><code>df_veiculos.createOrReplaceTempView(\"temp_veiculos\")\n</code></pre>"},{"location":"pipeline_gold/#da-merge-nos-dados-da-tabela-dimensional-e-temporaria_2","title":"Da merge nos dados da tabela dimensional e tempor\u00e1ria","text":"<pre><code>%sql\nMERGE INTO dim_veiculos AS dim\nUSING temp_veiculos AS temp\nON dim.codigo_veiculo = temp.codigo_veiculo \nWHEN MATCHED AND (dim.tipo_veiculo &lt;&gt; temp.tipo_veiculo OR dim.data_aquisicao &lt;&gt; temp.data_aquisicao OR dim.estado_veiculo &lt;&gt; temp.estado_veiculo OR dim.placa_veiculo &lt;&gt; temp.placa_veiculo) THEN\n  UPDATE SET \n    dim.tipo_veiculo = temp.tipo_veiculo,\n    dim.data_aquisicao = temp.data_aquisicao,\n    dim.estado_veiculo = temp.estado_veiculo,\n    dim.placa_veiculo = temp.placa_veiculo\n\nWHEN NOT MATCHED THEN\n  INSERT (codigo_veiculo, tipo_veiculo, data_aquisicao, estado_veiculo, placa_veiculo)\n  VALUES (temp.codigo_veiculo,temp.tipo_veiculo, temp.data_aquisicao, temp.estado_veiculo, temp.placa_veiculo);\n</code></pre>"},{"location":"pipeline_gold/#motoristas","title":"Motoristas","text":""},{"location":"pipeline_gold/#criando-tabela","title":"Criando Tabela","text":"<pre><code>%sql\n\nCREATE TABLE IF NOT EXISTS dim_motoristas (\n   sk_motoristas BIGINT GENERATED BY DEFAULT AS IDENTITY,\n   codigo_motorista BIGINT,\n   nome VARCHAR(255),\n   telefone VARCHAR(15),\n   numero_carteira VARCHAR(10),\n   data_contratacao DATE,\n   categoria_carteira VARCHAR(30),\n   status VARCHAR(30)\n)\nUSING delta\nLOCATION  '/mnt/datalake7a68c04c876ba15d/gold/dim_motoristas';\n</code></pre>"},{"location":"pipeline_gold/#criando-tabela-temporaria-de-motoristas","title":"Criando Tabela Tempor\u00e1ria de motoristas","text":"<pre><code>df_motoristas.createOrReplaceTempView(\"temp_motoristas\")\n</code></pre>"},{"location":"pipeline_gold/#da-merge-nos-dados-da-tabela-dimensional-e-temporaria_3","title":"Da merge nos dados da tabela dimensional e tempor\u00e1ria","text":"<pre><code>%sql\nMERGE INTO dim_motoristas AS dim\nUSING temp_motoristas AS temp\nON dim.codigo_motorista = temp.codigo_motorista \nWHEN MATCHED AND (dim.nome &lt;&gt; temp.nome OR dim.telefone &lt;&gt; temp.telefone OR dim.numero_carteira &lt;&gt; temp.numero_carteira OR dim.data_contratacao &lt;&gt; temp.data_contratacao OR dim.categoria_carteira &lt;&gt; temp.categoria_carteira OR dim.status &lt;&gt; temp.status) THEN\n  UPDATE SET \n    dim.nome = temp.nome,\n    dim.telefone = temp.telefone,\n    dim.numero_carteira = temp.numero_carteira,\n    dim.data_contratacao = temp.data_contratacao,\n    dim.categoria_carteira = temp.categoria_carteira,\n    dim.status = temp.status\n\nWHEN NOT MATCHED THEN\n  INSERT (codigo_motorista, nome, telefone, numero_carteira, data_contratacao, categoria_carteira, status)\n  VALUES (temp.codigo_motorista,temp.nome, temp.telefone, temp.numero_carteira, temp.data_contratacao, temp.categoria_carteira, temp.status);\n</code></pre>"},{"location":"pipeline_gold/#rotas","title":"Rotas","text":""},{"location":"pipeline_gold/#criando-tabela_1","title":"Criando Tabela","text":"<pre><code>%sql\n\nCREATE TABLE IF NOT EXISTS dim_rotas (\n   sk_rotas BIGINT GENERATED BY DEFAULT AS IDENTITY,\n   codigo_rota BIGINT,\n   origem CHAR(2),\n   destino CHAR(2),\n   tempo_estimado INT\n)\nUSING delta\nLOCATION  '/mnt/datalake7a68c04c876ba15d/gold/dim_rotas';\n</code></pre>"},{"location":"pipeline_gold/#criando-tabela-temporaria-de-rotas","title":"Criando Tabela Tempor\u00e1ria de rotas","text":"<pre><code>df_rotas.createOrReplaceTempView(\"temp_rotas\")\n</code></pre>"},{"location":"pipeline_gold/#da-merge-nos-dados-da-tabela-dimensional-e-temporaria_4","title":"Da merge nos dados da tabela dimensional e tempor\u00e1ria","text":"<pre><code>%sql\nMERGE INTO dim_rotas AS dim\nUSING temp_rotas AS temp\nON dim.codigo_rota = temp.codigo_rota \nWHEN MATCHED AND (dim.origem &lt;&gt; temp.origem OR dim.destino &lt;&gt; temp.destino OR dim.tempo_estimado &lt;&gt; temp.tempo_estimado) THEN\n  UPDATE SET \n    dim.origem = temp.origem,\n    dim.destino = temp.destino,\n    dim.tempo_estimado = temp.tempo_estimado\n\nWHEN NOT MATCHED THEN\n  INSERT (codigo_rota, origem, destino, tempo_estimado)\n  VALUES (temp.codigo_rota,temp.origem, temp.destino, temp.tempo_estimado);\n</code></pre>"},{"location":"pipeline_gold/#clientes","title":"Clientes","text":""},{"location":"pipeline_gold/#criando-tabela_2","title":"Criando Tabela","text":"<pre><code>%sql\n\nCREATE TABLE IF NOT EXISTS dim_clientes (\n   sk_clientes BIGINT GENERATED BY DEFAULT AS IDENTITY,\n   codigo_cliente BIGINT,\n   nome_cliente VARCHAR(255),\n   contato_cliente VARCHAR(15),\n   tipo_cliente VARCHAR(50),\n   logradouro_cliente VARCHAR(100),\n   numero_residencia_cliente VARCHAR(5),\n   bairro_cliente VARCHAR(100),\n   cep_cliente VARCHAR(10),\n   cidade_cliente VARCHAR(100),\n   uf_cliente CHAR(2)\n)\nUSING delta\nLOCATION  '/mnt/datalake7a68c04c876ba15d/gold/dim_clientes';\n</code></pre>"},{"location":"pipeline_gold/#criando-tabela-temporaria-de-clientes","title":"Criando tabela tempor\u00e1ria de clientes","text":"<pre><code>df_clientes.createOrReplaceTempView(\"temp_clientes\")\n</code></pre>"},{"location":"pipeline_gold/#da-merge-nos-dados-da-tabela-dimensional-e-temporaria_5","title":"Da merge nos dados da tabela dimensional e tempor\u00e1ria","text":"<pre><code>%sql\nMERGE INTO dim_clientes AS dim\nUSING temp_clientes AS temp\nON dim.codigo_cliente = temp.codigo_cliente \nWHEN MATCHED AND (dim.nome_cliente &lt;&gt; temp.nome_cliente OR dim.contato_cliente &lt;&gt; temp.contato_cliente OR dim.tipo_cliente &lt;&gt; temp.tipo_cliente OR dim.logradouro_cliente &lt;&gt; temp.logradouro_cliente OR dim.numero_residencia_cliente &lt;&gt; temp.numero_residencia_cliente OR dim.bairro_cliente &lt;&gt; temp.bairro_cliente OR dim.cep_cliente &lt;&gt; temp.cep_cliente OR dim.cidade_cliente &lt;&gt; temp.cidade_cliente OR dim.uf_cliente &lt;&gt; temp.uf_cliente ) THEN\n  UPDATE SET \n    dim.nome_cliente = temp.nome_cliente,\n    dim.contato_cliente = temp.contato_cliente,\n    dim.tipo_cliente = temp.tipo_cliente,\n    dim.logradouro_cliente = temp.logradouro_cliente,\n    dim.numero_residencia_cliente = temp.numero_residencia_cliente,\n    dim.bairro_cliente = temp.bairro_cliente,\n    dim.cep_cliente = temp.cep_cliente,\n    dim.cidade_cliente = temp.cidade_cliente,\n    dim.uf_cliente = temp.uf_cliente\n\nWHEN NOT MATCHED THEN\n  INSERT (codigo_cliente, nome_cliente, contato_cliente, tipo_cliente, logradouro_cliente, numero_residencia_cliente, bairro_cliente, cep_cliente, cidade_cliente, uf_cliente)\n  VALUES (temp.codigo_cliente,temp.nome_cliente, temp.contato_cliente, temp.tipo_cliente, temp.logradouro_cliente, temp.numero_residencia_cliente, temp.bairro_cliente, temp.cep_cliente, temp.cidade_cliente, temp.uf_cliente);\n</code></pre>"},{"location":"pipeline_gold/#tempo","title":"Tempo","text":""},{"location":"pipeline_gold/#criando-tabela_3","title":"Criando Tabela","text":"<pre><code>from pyspark.sql.functions import min, max, row_number\nfrom pyspark.sql import Window\n\nmenor_data = df_agendamentos.agg(min(\"data_hora_coleta\")).collect()[0][0]\n\nmaior_data = df_agendamentos.agg(max(\"data_hora_entrega\")).collect()[0][0]\n\nnum_dias = spark.sql(f\"SELECT datediff('{maior_data}', '{menor_data}')\").collect()[0][0]\n\ndf_calendario = spark.range(0, num_dias + 1) \\\n    .selectExpr(f\"date_add(to_date('{menor_data}'), CAST(id AS INT)) AS Data\")\n\ndf_tempo = df_calendario.selectExpr(\n    \"Data AS data\",\n    \"year(Data) AS ano\",\n    \"month(Data) AS mes\",\n       \"(CASE month(Data) \\\n        WHEN 1 THEN 'JANEIRO' \\\n        WHEN 2 THEN 'FEVEREIRO' \\\n        WHEN 3 THEN 'MARCO' \\\n        WHEN 4 THEN 'ABRIL' \\\n        WHEN 5 THEN 'MAIO' \\\n        WHEN 6 THEN 'JUNHO' \\\n        WHEN 7 THEN 'JULHO' \\\n        WHEN 8 THEN 'AGOSTO' \\\n        WHEN 9 THEN 'SETEMBRO' \\\n        WHEN 10 THEN 'OUTUBRO' \\\n        WHEN 11 THEN 'NOVEMBRO' \\\n        WHEN 12 THEN 'DEZEMBRO' \\\n    END) AS nome_mes\",\n    \"day(Data) AS dia\",\n    \"(CASE dayofweek(Data) \\\n        WHEN 1 THEN 'DOMINGO' \\\n        WHEN 2 THEN 'SEGUNDA-FEIRA' \\\n        WHEN 3 THEN 'TERCA-FEIRA' \\\n        WHEN 4 THEN 'QUARTA-FEIRA' \\\n        WHEN 5 THEN 'QUINTA-FEIRA' \\\n        WHEN 6 THEN 'SEXTA-FEIRA' \\\n        WHEN 7 THEN 'SABADO' \\\n    END) AS nome_dia_semana\",\n    \"dayofweek(Data) AS numero_dia_semana\"\n)\n\nwindowSpec = Window.orderBy(\"data\")\ndf_tempo = df_tempo.withColumn(\"sk_tempo\", row_number().over(windowSpec))\n\ndf_tempo.write.mode(\"overwrite\") \\\n    .option(\"path\", f\"/mnt/{storageAccountName}/gold/dim_tempo\") \\\n    .saveAsTable(\"dim_tempo\", format=\"delta\")\n</code></pre>"},{"location":"pipeline_gold/#tabela-fato-entregas","title":"Tabela Fato (Entregas)","text":""},{"location":"pipeline_gold/#criando-tabela_4","title":"Criando Tabela","text":"<pre><code>%sql\nCREATE TABLE IF NOT EXISTS fato_entregas (\n   sk_entrega BIGINT GENERATED BY DEFAULT AS IDENTITY,\n   fk_cargas BIGINT,\n   fk_veiculos BIGINT,\n   fk_motoristas BIGINT,\n   fk_rotas BIGINT,\n   fk_clientes BIGINT,\n   fk_data_coleta BIGINT,\n   fk_data_entrega BIGINT,\n   fk_data_prevista_entrega BIGINT,\n   distancia_percorrida NUMERIC(10,2),\n   tempo_entrega NUMERIC(10,2),\n   volume_carga NUMERIC(10,2)\n)\nUSING delta\nLOCATION '/mnt/datalake7a68c04c876ba15d/gold/fato_entregas';\n</code></pre>"},{"location":"pipeline_gold/#adicionando-tempo_entrega-e-alterando-data_hora-para-somente-data","title":"Adicionando tempo_entrega e alterando data_hora para somente data","text":"<pre><code>from pyspark.sql.types import TimestampType\nfrom pyspark.sql.functions import unix_timestamp, col, to_date\n\ndf_agendamentos_fato = df_agendamentos \\\n  .withColumn(\"data_hora_coleta\" ,\n              df_agendamentos[\"data_hora_coleta\"]\n              .cast(TimestampType()))   \\\n  .withColumn(\"data_hora_entrega\",\n              df_agendamentos[\"data_hora_entrega\"]\n              .cast(TimestampType()))\n\n\ndf_agendamentos_fato = df_agendamentos_fato.withColumn(\"coleta_segundos\", unix_timestamp(col(\"data_hora_coleta\"))) \\\n    .withColumn(\"entrega_segundos\", unix_timestamp(col(\"data_hora_entrega\"))) \\\n        .withColumn(\"diferenca_segundos\", col(\"entrega_segundos\") - col(\"coleta_segundos\"))\n\ndf_agendamentos_fato = df_agendamentos_fato.withColumn(\"tempo_entrega\", col(\"diferenca_segundos\") / 3600)\n\n\n\ndf_agendamentos_fato = df_agendamentos_fato.withColumn(\"data_coleta\", to_date(\"data_hora_coleta\")) \\\n  .withColumn(\"data_entrega\", to_date(\"data_hora_entrega\"))\n\ndf_agendamentos_fato = df_agendamentos_fato.drop(\"entrega_segundos\", \"diferenca_segundos\", \"coleta_segundos\", \"data_hora_entrega\", \"data_hora_coleta\")\n\ndf_agendamentos_fato.display()\n</code></pre>"},{"location":"pipeline_gold/#adicionando-codigo_motorista-ao-dataframe-agendamentos","title":"Adicionando codigo_motorista ao DataFrame agendamentos","text":"<pre><code>df_agendamentos_fato = df_agendamentos_fato.join(\n    df_veiculos.select(\"codigo_veiculo\", \"codigo_motorista\"),\n    on=\"codigo_veiculo\",\n    how=\"left\"\n)\n</code></pre>"},{"location":"pipeline_gold/#populando-a-tabela-fato","title":"Populando a tabela fato","text":"<pre><code>df_agendamentos_fato.createOrReplaceTempView(\"temp_agendamentos_fato\")\n</code></pre> <pre><code>%sql\nwith cte as (\n    select tc.codigo_carga, tem.sk_tempo \n    from temp_cargas tc \n    inner join dim_tempo tem \n    on tc.data_entrega_prevista = tem.data\n)\ninsert into fato_entregas (\n    fk_cargas, \n    fk_veiculos, \n    fk_motoristas, \n    fk_rotas, \n    fk_clientes, \n    fk_data_coleta, \n    fk_data_entrega, \n    fk_data_prevista_entrega, \n    distancia_percorrida, \n    tempo_entrega,\n    volume_carga\n    )\nselect  \n        car.sk_cargas,\n        ve.sk_veiculos,\n        mo.sk_motoristas,\n        ro.sk_rotas,\n        cli.sk_clientes,\n        coleta.sk_tempo,\n        entrega.sk_tempo,\n        cte.sk_tempo,\n        tr.distancia,\n        fato.tempo_entrega,\n        car.comprimento * car.altura * car.largura\nfrom temp_agendamentos_fato fato\n       inner join dim_cargas car\n         on car.codigo_carga = fato.codigo_carga\n       inner join dim_veiculos ve\n         on ve.codigo_veiculo = fato.codigo_veiculo\n       inner join dim_motoristas mo\n         on mo.codigo_motorista = fato.codigo_motorista\n       inner join dim_rotas ro\n         on ro.codigo_rota = fato.codigo_rota\n       inner join dim_clientes cli\n         on cli.codigo_cliente = fato.codigo_cliente\n       inner join cte\n         on cte.codigo_carga = fato.codigo_carga\n       inner join dim_tempo coleta\n         on coleta.data = fato.data_coleta\n       inner join dim_tempo entrega\n         on entrega.data = fato.data_entrega\n       inner join temp_rotas tr\n         on tr.codigo_rota = fato.codigo_rota\n</code></pre>"},{"location":"pipeline_silver/","title":"Camada Silver","text":"<p>A camada Silver \u00e9 respons\u00e1vel pela limpeza e transforma\u00e7\u00e3o inicial dos dados brutos da camada Bronze. Aqui, aplicamos filtros, valida\u00e7\u00f5es e ajustes para garantir que os dados estejam estruturados de maneira consistente e prontos para an\u00e1lises mais complexas. Utilizamos PySpark para processar e manipular os dados nesta etapa.</p>"},{"location":"pipeline_silver/#mostrando-todos-os-arquivos-da-camada-bronze","title":"Mostrando todos os arquivos da camada bronze","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/bronze\"))\n</code></pre>"},{"location":"pipeline_silver/#gerando-um-dataframe-dos-delta-lake-no-container-bronze-do-azure-data-lake-storage","title":"Gerando um dataframe dos delta lake no container bronze do Azure Data Lake Storage","text":"<pre><code>df_agendamentos = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/agendamentos\")\ndf_cargas = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/cargas\")\ndf_clientes = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/clientes\")\ndf_motoristas = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/motoristas\")\ndf_rotas = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/rotas\")\ndf_veiculos = spark.read.format('delta').load(f\"/mnt/{storageAccountName}/bronze/veiculos\")\n</code></pre>"},{"location":"pipeline_silver/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_agendamentos   = df_agendamentos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"agendamentos\"))\ndf_cargas     = df_cargas.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"cargas\"))\ndf_clientes   = df_clientes.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"clientes\"))\ndf_motoristas  = df_motoristas.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"motoristas\"))\ndf_rotas    = df_rotas.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"rotas\"))\ndf_veiculos     = df_veiculos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"veiculos\"))\n</code></pre>"},{"location":"pipeline_silver/#mudando-as-colunas-para-conter-somente-letras-minusculas-e-retirando-abreviacoes","title":"Mudando as colunas para conter somente letras min\u00fasculas e retirando abrevia\u00e7\u00f5es","text":"<pre><code>import re\n\ndef atualizar_colunas(df):\n    colunas = df.columns\n    for coluna in colunas:\n        if coluna.lower() == coluna:\n            continue\n        nova_coluna = coluna\n        if \"ID\" in nova_coluna:\n            position = nova_coluna.find(\"ID\")\n            nova_coluna = \"Codigo\"+nova_coluna[:position]\n        nova_coluna = \"_\".join(re.findall('[A-Z][^A-Z]*', nova_coluna)).lower()\n        df = df.withColumnRenamed(coluna, nova_coluna)\n    return df\n\ndf_agendamentos = atualizar_colunas(df_agendamentos)\ndf_cargas = atualizar_colunas(df_cargas)\ndf_clientes = atualizar_colunas(df_clientes)\ndf_motoristas = atualizar_colunas(df_motoristas)\ndf_rotas = atualizar_colunas(df_rotas)\ndf_veiculos = atualizar_colunas(df_veiculos)\n</code></pre>"},{"location":"pipeline_silver/#separando-endereco-do-cliente","title":"Separando endere\u00e7o do cliente","text":"<pre><code>import pyspark.pandas as ps\nps.set_option('compute.ops_on_diff_frames', True)\n\npd_clientes = ps.DataFrame(df_clientes)\n\ndef separar_endereco(r):\n    endereco = r['endereco_cliente']\n    endereco = endereco.split(',')\n    if len(endereco) == 4:\n        logradouro = endereco[0].strip()\n        numero_residencia = endereco[1].strip()\n        bairro = endereco[2].strip()\n        cep_cidade_uf = endereco[3].strip()\n    else:\n        logradouro = endereco[0].strip()\n        numero_residencia = \"S/N\"\n        bairro = endereco[1].strip()\n        cep_cidade_uf = endereco[2].strip()\n\n    cep_cidade_uf = cep_cidade_uf.split('/')\n    cep_cidade = cep_cidade_uf[0].strip()\n    uf = cep_cidade_uf[1].strip()\n    cep = cep_cidade.split(' ')[0].strip()\n    cep = ''.join(cep.split('-'))\n    cidade = cep_cidade[len(cep) + 1:].strip()\n\n    return [logradouro, numero_residencia, bairro, cep, cidade, uf]\n\nendereco = pd_clientes.apply(separar_endereco, axis=1)\n\nendereco_df = ps.DataFrame(endereco.tolist(), columns=['logradouro_cliente', 'numero_residencia_cliente', 'bairro_cliente', 'cep_cliente', 'cidade_cliente', 'uf_cliente'])\npd_clientes = ps.concat([pd_clientes, endereco_df], axis=1)\npd_clientes = pd_clientes.drop(columns=\"endereco_cliente\")\npd_clientes.columns\n</code></pre>"},{"location":"pipeline_silver/#padronizando-colunas-de-contato-para-somente-numero","title":"Padronizando colunas de contato para somente n\u00famero","text":"<pre><code>def padronizar_contato(r, coluna):\n    contato = r[coluna]\n    contato = re.sub('[^0-9]', '', contato)\n    return contato\n\npd_clientes[\"contato_cliente\"] = pd_clientes.apply(lambda row: padronizar_contato(row, \"contato_cliente\"), axis=1)\n\npd_motoristas = ps.DataFrame(df_motoristas)\npd_motoristas[\"telefone\"] = pd_motoristas.apply(lambda row: padronizar_contato(row, \"telefone\"), axis=1)\n</code></pre>"},{"location":"pipeline_silver/#mudando-tempo_estimado-para-minutos","title":"Mudando tempo_estimado para minutos","text":"<pre><code>def tempo_em_minutos(r):\n    tempo_estimado = r[\"tempo_estimado\"]\n    tempo_estimado = tempo_estimado.split(\":\")\n    horas = int(tempo_estimado[0])\n    minutos = int(tempo_estimado[1])\n    minutos += horas * 60\n    return minutos\n\npd_rotas = ps.DataFrame(df_rotas)\npd_rotas[\"tempo_estimado\"] = pd_rotas.apply(tempo_em_minutos, axis=1)\ndf_rotas = pd_rotas.to_spark()\ndf_rotas.select(\"tempo_estimado\").show(5)\n</code></pre>"},{"location":"pipeline_silver/#ajustando-tipo-dos-dados","title":"Ajustando tipo dos dados","text":""},{"location":"pipeline_silver/#visualizando-tipos","title":"Visualizando tipos","text":"<p><pre><code>print(\"agendamentos: \", df_agendamentos.describe(), \"\\n\")\nprint(\"cargas: \", df_cargas.describe(), \"\\n\") \nprint(\"clientes: \", df_clientes.describe(), \"\\n\") \nprint(\"motoristas: \", df_motoristas.describe(), \"\\n\") \nprint(\"rotas: \", df_rotas.describe(), \"\\n\") \nprint(\"veiculos: \", df_veiculos.describe(), \"\\n\")\n</code></pre> </p>"},{"location":"pipeline_silver/#ajustando-tipagem-de-cargas","title":"Ajustando Tipagem de Cargas","text":"<pre><code>from pyspark.sql.types import FloatType\ndf_cargas = df_cargas \\\n  .withColumn(\"peso_carga\" ,\n              df_cargas[\"peso_carga\"]\n              .cast(FloatType()))   \\\n  .withColumn(\"comprimento\",\n              df_cargas[\"comprimento\"]\n              .cast(FloatType()))    \\\n  .withColumn(\"largura\"  ,\n              df_cargas[\"largura\"]\n              .cast(FloatType())) \\\n  .withColumn(\"altura\"  ,\n              df_cargas[\"altura\"]\n              .cast(FloatType())) \n</code></pre>"},{"location":"pipeline_silver/#ajustando-tipagem-de-rotas","title":"Ajustando Tipagem de Rotas","text":"<pre><code>df_rotas = df_rotas \\\n  .withColumn(\"distancia\" ,\n              df_rotas[\"distancia\"]\n              .cast(FloatType()))  \n</code></pre>"},{"location":"pipeline_silver/#apagando-coluna-nula-do-dataframe-rotas","title":"Apagando coluna nula do DataFrame Rotas","text":"<pre><code>df_rotas = df_rotas.drop(\"restricoes_trafego\")\ndf_rotas.columns\n</code></pre>"},{"location":"pipeline_silver/#salvando-os-dataframes-em-delta-lake-formato-de-arquivo-no-data-lake-repositorio-cloud","title":"Salvando os dataframes em delta lake (formato de arquivo) no data lake (repositorio cloud)","text":"<pre><code>df_agendamentos.write.format('delta').mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"/mnt/{storageAccountName}/silver/agendamentos\"),\ndf_cargas.write.format('delta').mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"/mnt/{storageAccountName}/silver/cargas\"),\ndf_clientes.write.format('delta').mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"/mnt/{storageAccountName}/silver/clientes\"),\ndf_motoristas.write.format('delta').mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"/mnt/{storageAccountName}/silver/motoristas\"),\ndf_rotas.write.format('delta').mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"/mnt/{storageAccountName}/silver/rotas\"),\ndf_veiculos.write.format('delta').mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(f\"/mnt/{storageAccountName}/silver/veiculos\")\n</code></pre>"},{"location":"pipeline_silver/#verificando-os-dados-gravados-em-delta-na-camada-silver","title":"Verificando os dados gravados em delta na camada silver","text":"<p><pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/silver/\"))\n ```\n\n### Lendo um exemplo de um delta lake para validar a existencia dos dados e das colunas do metadados\n\n```python\nspark.read.format('delta').load(f'/mnt/{storageAccountName}/silver/agendamentos').limit(10).display()\n</code></pre> </p>"},{"location":"sobre/","title":"Sobre","text":""},{"location":"sobre/#formatacao-de-texto","title":"Formatacao de texto","text":"<p>Text can be deleted and replacement text added. This can also be combined into onea single operation. Highlighting is also possible and comments can be added inline.</p> <p>Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.</p>"},{"location":"sobre/#blocos","title":"Blocos","text":"<ul> <li> Aluno 1 Fulano de tal</li> <li> Aluno 2 Sicrano</li> <li> Aluno 3 Beltrano</li> <li> Aluno 4 ... huh?</li> </ul> <p>Phasellus posuere in sem ut cursus</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> Failure <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"}]}