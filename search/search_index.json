{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projeto Sistema de Transportadora Inteligente","text":""},{"location":"#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Este projeto foi desenvolvido para implementar um sistema de gest\u00e3o inteligente para uma transportadora, utilizando t\u00e9cnicas avan\u00e7adas de Engenharia de Dados para otimizar opera\u00e7\u00f5es log\u00edsticas e melhorar a efici\u00eancia de entregas, processos e de gerenciamento interno. A equipe aplicou ferramentas modernas e tecnologias como Python, PySpark, Delta Lake, e Databricks para construir uma robusta pipeline de dados.</p>"},{"location":"#objetivo","title":"Objetivo","text":"<p>O objetivo principal deste projeto \u00e9 desenvolver um sistema de gest\u00e3o integrada para uma transportadora, utilizando engenharia de dados para otimizar opera\u00e7\u00f5es log\u00edsticas. Isso inclui a coleta, armazenamento, transforma\u00e7\u00e3o e an\u00e1lise de dados. A equipe buscar\u00e1 melhorar a efici\u00eancia na atribui\u00e7\u00e3o de motoristas e ve\u00edculos, planejamento de rotas otimizado, gest\u00e3o eficaz de cargas e melhor experi\u00eancia para os clientes, garantindo entregas pontuais e seguras.</p>"},{"location":"#equipe","title":"Equipe","text":"<p>A equipe respons\u00e1vel pelo desenvolvimento deste projeto inclui:</p> <ul> <li>Henrique Forgiarini - Coleta e integra\u00e7\u00e3o de dados de rastreamento - Perfil GitHub</li> <li>Bruno Supriano - Armazenamento e gerenciamento de dados no Delta Lake - Perfil GitHub</li> <li>Renato Ribas - Configura\u00e7\u00e3o e otimiza\u00e7\u00e3o do ambiente no Databricks - Perfil GitHub</li> <li>Tiago Salles - Desenvolvimento de modelos preditivos para gest\u00e3o de frota - Perfil GitHub</li> <li>Jo\u00e3o Pedro Cardoso - An\u00e1lise de dados utilizando PySpark - Perfil GitHub</li> <li>Diego Hahn - Orquestra\u00e7\u00e3o de workflows no Databricks - Perfil GitHub</li> <li>Jhayne Henemam - Desenvolvimento de dashboards para visualiza\u00e7\u00e3o de dados - Perfil GitHub</li> <li>Keniel Alves - Gerenciamento de versionamento e documenta\u00e7\u00e3o - Perfil GitHub</li> </ul>"},{"location":"#pipeline-de-engenharia-de-dados","title":"Pipeline de Engenharia de Dados","text":"<p>A pipeline desenvolvida neste projeto inclui as seguintes etapas:</p> <ol> <li>Coleta de Dados: Utiliza\u00e7\u00e3o de uma biblioteca de inser\u00e7\u00e3o de dados fict\u00edcios (Faker).</li> <li>Armazenamento de Dados: Utiliza\u00e7\u00e3o do Delta Lake para armazenar dados de forma escal\u00e1vel e confi\u00e1vel.</li> <li>Transforma\u00e7\u00e3o de Dados: Processamento de grandes volumes de dados utilizando PySpark no Databricks.</li> <li>An\u00e1lise de Dados: Utiliza\u00e7\u00e3o de notebooks interativos no Databricks para an\u00e1lise preditiva e otimiza\u00e7\u00e3o de rotas.</li> <li>Automa\u00e7\u00e3o e Orquestra\u00e7\u00e3o: Orquestra\u00e7\u00e3o de workflows no Databricks para automatizar processos de ingest\u00e3o, transforma\u00e7\u00e3o e an\u00e1lise de dados.</li> </ol>"},{"location":"#tecnologias-utilizadas","title":"Tecnologias Utilizadas","text":"<p>Al\u00e9m das tecnologias mencionadas, este projeto tamb\u00e9m fez uso extensivo das capacidades do Databricks para processamento de big data em tempo real e colabora\u00e7\u00e3o em equipe.</p>"},{"location":"#conclusao","title":"Conclus\u00e3o","text":"<p>Este projeto n\u00e3o apenas demonstra a aplica\u00e7\u00e3o pr\u00e1tica dos conceitos de Engenharia de Dados, mas tamb\u00e9m oferece uma solu\u00e7\u00e3o inovadora para melhorar a efici\u00eancia e a competitividade de uma transportadora. A equipe est\u00e1 entusiasmada em contribuir para um projeto que pode transformar positivamente a log\u00edstica de transporte de cargas.</p> Copyright \u00a9 2024 - Jorge Luiz da Silva - Todos os direitos reservados."},{"location":"pipeline/","title":"Pipeline de Dados","text":""},{"location":"pipeline/#introducao","title":"Introdu\u00e7\u00e3o","text":"<p>Nesta p\u00e1gina, vamos explorar a pipeline de dados desenvolvida para a transportadora, abordando como foi estruturada e as etapas envolvidas desde a coleta at\u00e9 a entrega de dados prontos para an\u00e1lise.</p>"},{"location":"pipeline/#estrutura-da-pipeline","title":"Estrutura da Pipeline","text":"<p>A pipeline de dados foi organizada em tr\u00eas camadas principais: Bronze, Silver e Gold. Cada camada desempenha um papel espec\u00edfico na prepara\u00e7\u00e3o e transforma\u00e7\u00e3o dos dados.</p>"},{"location":"pipeline/#conectando-azure-adls-gen2-no-databricks","title":"Conectando Azure ADLS Gen2 no Databricks","text":""},{"location":"pipeline/#definindo-storage-account-e-sas-key","title":"Definindo storage account e sas key","text":"<pre><code>storageAccountName = \"datalake7a68c04c876ba15d\"\nsasToken = dbutils.secrets.get(scope=\"sas-token\", key=\"sas-tkn\")\n</code></pre>"},{"location":"pipeline/#definindo-uma-funcao-para-montar-um-adls-com-um-ponto-de-montagem-com-adls-sas","title":"Definindo uma fun\u00e7\u00e3o para montar um ADLS com um ponto de montagem com ADLS SAS","text":"<pre><code>def mount_adls(blobContainerName):\n    try:\n      dbutils.fs.mount(\n        source = \"wasbs://{}@{}.blob.core.windows.net\".format(blobContainerName, storageAccountName),\n        mount_point = f\"/mnt/{storageAccountName}/{blobContainerName}\",\n        extra_configs = {'fs.azure.sas.' + blobContainerName + '.' + storageAccountName + '.blob.core.windows.net': sasToken}\n      )\n      print(\"OK!\")\n    except Exception as e:\n      print(\"Falha\", e)\n</code></pre>"},{"location":"pipeline/#montando-todos-os-containers","title":"Montando todos os containers","text":"<pre><code>mount_adls('landing-zone')\nmount_adls('bronze')\nmount_adls('silver')\nmount_adls('gold')\n</code></pre>"},{"location":"pipeline/#mostrando-os-pontos-de-montagem-no-cluster-databricks","title":"Mostrando os pontos de montagem no cluster Databricks","text":"<pre><code>display(dbutils.fs.mounts())\n</code></pre>"},{"location":"pipeline/#transformacoes-de-dados","title":"Transforma\u00e7\u00f5es de dados","text":""},{"location":"pipeline/#mostrando-todos-os-arquivos-da-camada-landing-zone","title":"Mostrando todos os arquivos da camada landing-zone","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/landing-zone\"))\n</code></pre>"},{"location":"pipeline/#gerando-um-dataframe-para-cada-arquivo-a-partir-dos-arquivos-csv-gravado-no-container-landing-zone-do-azure-data-lake-storage","title":"Gerando um dataframe para cada arquivo a partir dos arquivos CSV gravado no container landing-zone do Azure Data Lake Storage","text":"<pre><code>df_agendamentos = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Agendamentos.csv\")\ndf_cargas = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Cargas.csv\")\ndf_clientes = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Clientes.csv\")\ndf_motoristas = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Motoristas.csv\") \ndf_rotas = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Rotas.csv\")\ndf_veiculos = spark.read.option(\"infeschema\", \"true\").option(\"header\", \"true\").csv(f\"/mnt/{storageAccountName}/landing-zone/Veiculos.csv\")\n</code></pre>"},{"location":"pipeline/#adicionando-metadados-de-data-e-hora-de-processamento-e-nome-do-arquivo-de-origem","title":"Adicionando metadados de data e hora de processamento e nome do arquivo de origem","text":"<pre><code>from pyspark.sql.functions import current_timestamp, lit\n\ndf_agendamentos = df_agendamentos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Agendamentos.csv\"))\ndf_cargas = df_cargas.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Cargas.csv\"))\ndf_clientes = df_clientes.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Clientes.csv\"))\ndf_motoristas = df_motoristas.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Motoristas.csv\"))\ndf_rotas = df_rotas.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Rotas.csv\"))\ndf_veiculos = df_veiculos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"Veiculos.csv\"))\n</code></pre>"},{"location":"pipeline/#salvando-os-dataframes-em-delta-lake-formato-de-arquivo-no-data-lake-repositorio-cloud","title":"Salvando os dataframes em delta lake (formato de arquivo) no data lake (repositorio cloud)","text":"<pre><code>df_agendamentos.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/agendamentos\")\ndf_cargas.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/cargas\")\ndf_clientes.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/clientes\")\ndf_motoristas.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/motoristas\")\ndf_rotas.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/rotas\")\ndf_veiculos.write.format('delta').mode(\"overwrite\").save(f\"/mnt/{storageAccountName}/bronze/veiculos\")\n</code></pre>"},{"location":"pipeline/#verificando-os-dados-gravados-em-delta-na-camada-bronze","title":"Verificando os dados gravados em delta na camada bronze","text":"<pre><code>display(dbutils.fs.ls(f\"/mnt/{storageAccountName}/bronze/\"))\n</code></pre>"},{"location":"pipeline/#lendo-um-exemplo-de-um-delta-lake-para-validar-a-existencia-dos-dados-e-das-colunas-do-metadados","title":"Lendo um exemplo de um delta lake para validar a existencia dos dados e das colunas do metadados","text":"<pre><code>spark.read.format('delta').load(f'/mnt/{storageAccountName}/bronze/veiculos').limit(10).display()\n</code></pre>"},{"location":"pipeline/#camada-bronze","title":"Camada Bronze","text":"<p>Na camada Bronze, os dados brutos s\u00e3o coletados e armazenados. Isso inclui informa\u00e7\u00f5es diretamente obtidas das tabelas principais. A integridade e a fidelidade dos dados s\u00e3o preservadas nesta etapa inicial.</p>"},{"location":"pipeline/#camada-silver","title":"Camada Silver","text":"<p>A camada Silver \u00e9 respons\u00e1vel pela limpeza e transforma\u00e7\u00e3o inicial dos dados brutos da camada Bronze. Aqui, aplicamos filtros, valida\u00e7\u00f5es e ajustes para garantir que os dados estejam estruturados de maneira consistente e prontos para an\u00e1lises mais complexas. Utilizamos PySpark para processar e manipular os dados nesta etapa.</p> <p>Exemplo de c\u00f3digo em PySpark para transforma\u00e7\u00e3o na camada Silver:</p> <pre><code># Exemplo de c\u00f3digo PySpark para transforma\u00e7\u00e3o na camada Silver\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when\n\n# Inicializa\u00e7\u00e3o da sess\u00e3o Spark\nspark = SparkSession.builder.appName(\"PipelineSilver\").getOrCreate()\n\n# Carregamento dos dados da camada Bronze\nbronze_data = spark.read.csv(\"caminho/para/bronze_data/*.csv\", header=True)\n\n# Aplica\u00e7\u00e3o de transforma\u00e7\u00f5es\nsilver_data = bronze_data.withColumn(\"nova_coluna\", when(col(\"condi\u00e7\u00e3o\"), \"valor_true\").otherwise(\"valor_false\"))\n\n# Salvar dados transformados na camada Silver\nsilver_data.write.mode(\"overwrite\").parquet(\"caminho/para/silver_data\")\n</code></pre>"},{"location":"sobre/","title":"Sobre","text":""},{"location":"sobre/#formatacao-de-texto","title":"Formatacao de texto","text":"<p>Text can be deleted and replacement text added. This can also be combined into onea single operation. Highlighting is also possible and comments can be added inline.</p> <p>Formatting can also be applied to blocks by putting the opening and closing tags on separate lines and adding new lines between the tags and the content.</p>"},{"location":"sobre/#blocos","title":"Blocos","text":"<ul> <li> Aluno 1 Fulano de tal</li> <li> Aluno 2 Sicrano</li> <li> Aluno 3 Beltrano</li> <li> Aluno 4 ... huh?</li> </ul> <p>Phasellus posuere in sem ut cursus</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Note</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p> Failure <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"}]}